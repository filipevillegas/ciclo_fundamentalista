{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMeeIkejSyqQ7IXcstx55E9",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/filipevillegas/ciclo_fundamentalista/blob/main/ciclo_fundamentalista.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LtFGa20Vby7a",
    "outputId": "03d5b770-da16-4820-94d5-96f62f6f56ee"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "====================================================================================================\n",
      " SCCS ANALYSIS PIPELINE v4.0 - PROFESSIONAL EDITION\n",
      "====================================================================================================\n",
      "\n",
      "üì¶ Etapa 1: Verificando depend√™ncias...\n",
      "‚úì gspread j√° instalado\n",
      "üì¶ Instalando google-auth...\n",
      "‚úì google-auth instalado com sucesso\n",
      "üì¶ Instalando google-colab...\n",
      "‚úì google-colab instalado com sucesso\n",
      "‚úì pandas j√° instalado\n",
      "‚úì numpy j√° instalado\n",
      "‚úì matplotlib j√° instalado\n",
      "‚úì seaborn j√° instalado\n",
      "‚úì scipy j√° instalado\n",
      "‚úì yfinance j√° instalado\n",
      "üì¶ Instalando pandas-ta...\n",
      "‚úì pandas-ta instalado com sucesso\n",
      "üì¶ Instalando quantstats...\n",
      "‚úì quantstats instalado com sucesso\n",
      "\n",
      "üìä Etapa 2: Carregando e validando dados...\n",
      "üîê Autenticando com Google Sheets...\n",
      "üìÑ Acessando planilha ID: 1SWNjeO7Dc...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR:__main__:ERROR in load_and_prepare_data: \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gspread/client.py\", line 168, in open_by_key\n",
      "    spreadsheet = Spreadsheet(self.http_client, {\"id\": key})\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gspread/spreadsheet.py\", line 29, in __init__\n",
      "    metadata = self.fetch_sheet_metadata()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gspread/spreadsheet.py\", line 230, in fetch_sheet_metadata\n",
      "    return self.client.fetch_sheet_metadata(self.id, params=params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gspread/http_client.py\", line 305, in fetch_sheet_metadata\n",
      "    r = self.request(\"get\", url, params=params)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gspread/http_client.py\", line 128, in request\n",
      "    raise APIError(response)\n",
      "gspread.exceptions.APIError: APIError: [403]: The caller does not have permission\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-2635954476.py\", line 1033, in load_and_prepare_data\n",
      "    spreadsheet = gc_client.open_by_key(sheet_id)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gspread/client.py\", line 173, in open_by_key\n",
      "    raise PermissionError from ex\n",
      "PermissionError\n",
      "ERROR:__main__:ERROR in run_complete_pipeline: Falha ao carregar dados\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-2635954476.py\", line 1446, in run_complete_pipeline\n",
      "    raise Exception(\"Falha ao carregar dados\")\n",
      "Exception: Falha ao carregar dados\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "‚ùå Erro ao carregar dados: \n",
      "\n",
      "‚ùå ERRO CR√çTICO: Falha ao carregar dados\n",
      "\n",
      "‚ùå Erro na execu√ß√£o: Falha ao carregar dados\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gspread\n",
    "from google.auth import default\n",
    "from google.auth.exceptions import DefaultCredentialsError\n",
    "from gspread.exceptions import SpreadsheetNotFound, WorksheetNotFound\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import CubicSpline\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "import importlib\n",
    "import importlib.util\n",
    "import gc\n",
    "import hashlib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "COLAB_AUTH = None\n",
    "_colab_spec = importlib.util.find_spec(\"google.colab\")\n",
    "if _colab_spec:\n",
    "    from google.colab import auth as _colab_auth  # type: ignore\n",
    "    COLAB_AUTH = _colab_auth\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURA√á√ÉO CENTRALIZADA\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configura√ß√£o centralizada de par√¢metros do sistema\"\"\"\n",
    "\n",
    "    # Pesos SCCS - ajustados para dados dispon√≠veis\n",
    "    SCCS_WEIGHTS: Dict[str, float] = field(default_factory=lambda: {\n",
    "        'SALES_GROWTH': 0.30,        # Aumentado\n",
    "        'RETURN_COM_EQY': 0.25,      # Mantido\n",
    "        'OPER_MARGIN': 0.20,         # Mantido\n",
    "        'CF_FREE_CASH_FLOW': 0.15,   # Mantido\n",
    "        'OPERATING_ROIC': 0.10,      # Ajustado\n",
    "    })\n",
    "\n",
    "    # Thresholds para scoring\n",
    "    THRESHOLDS: Dict[str, Dict] = field(default_factory=lambda: {\n",
    "        'SALES_GROWTH': {'neutral': 5, 'worst': -15, 'best': 30},\n",
    "        'RETURN_COM_EQY': {'neutral': 12.5, 'scale': 10},\n",
    "        'OPER_MARGIN': {'neutral': 10, 'worst': -5, 'best': 25},\n",
    "        'OPERATING_ROIC': {'neutral': 8, 'worst': -5, 'best': 35},\n",
    "        'CF_FREE_CASH_FLOW': {'neutral': 5, 'worst': -5, 'best': 15}\n",
    "    })\n",
    "\n",
    "    # Configura√ß√µes de visualiza√ß√£o\n",
    "    PLOT_CONFIG: Dict = field(default_factory=lambda: {\n",
    "        'figure_size': (14, 9),\n",
    "        'lookback_periods': 16,\n",
    "        'min_data_points': 3,\n",
    "        'dpi': 100\n",
    "    })\n",
    "\n",
    "    # Configura√ß√µes de valida√ß√£o\n",
    "    VALIDATION: Dict = field(default_factory=lambda: {\n",
    "        'max_gap_days': 120,\n",
    "        'outlier_std_threshold': 3,\n",
    "        'min_data_points': 8,\n",
    "        'lookback_quarters': 4\n",
    "    })\n",
    "\n",
    "    # URLs e credenciais\n",
    "    SHEET_URL: Optional[str] = None\n",
    "    LOG_LEVEL: str = 'INFO'\n",
    "    OUTPUT_FORMAT: str = 'csv'  # csv, excel, parquet\n",
    "    GOOGLE_SERVICE_ACCOUNT_FILE: Optional[str] = None\n",
    "    GOOGLE_WORKSHEET_NAME: Optional[str] = None\n",
    "\n",
    "# =============================================================================\n",
    "# SISTEMA DE LOGGING FINANCEIRO\n",
    "# =============================================================================\n",
    "class FinancialLogger:\n",
    "    \"\"\"Sistema de logging para auditoria de c√°lculos financeiros\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        log_filename = f'sccs_audit_{datetime.now():%Y%m%d_%H%M%S}.log'\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=getattr(logging, config.LOG_LEVEL),\n",
    "            format='%(asctime)s | %(levelname)s | %(funcName)s | %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_filename),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.audit_trail = []\n",
    "\n",
    "    def log_calculation(self, empresa: str, metric: str, value: float, details: Dict = None):\n",
    "        \"\"\"Registra c√°lculo para auditoria\"\"\"\n",
    "        msg = f\"[{empresa}] {metric}: {value:.4f}\"\n",
    "        if details:\n",
    "            msg += f\" | Details: {json.dumps(details, default=str)}\"\n",
    "        self.logger.info(msg)\n",
    "\n",
    "        self.audit_trail.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'empresa': empresa,\n",
    "            'metric': metric,\n",
    "            'value': value,\n",
    "            'details': details\n",
    "        })\n",
    "\n",
    "    def log_anomaly(self, empresa: str, issue: str, data: Any):\n",
    "        \"\"\"Registra anomalias detectadas\"\"\"\n",
    "        self.logger.warning(f\"[{empresa}] ANOMALY: {issue} | Data: {data}\")\n",
    "\n",
    "    def log_error(self, context: str, error: Exception):\n",
    "        \"\"\"Registra erros com contexto\"\"\"\n",
    "        self.logger.error(f\"ERROR in {context}: {str(error)}\", exc_info=True)\n",
    "\n",
    "    def export_audit_trail(self, filename: str = 'audit_trail.json'):\n",
    "        \"\"\"Exporta trilha de auditoria\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.audit_trail, f, indent=2, default=str)\n",
    "        self.logger.info(f\"Audit trail exported to {filename}\")\n",
    "\n",
    "# =============================================================================\n",
    "# INSTALADOR INTELIGENTE DE PACOTES\n",
    "# =============================================================================\n",
    "class PackageManager:\n",
    "    \"\"\"Gerenciador inteligente de pacotes com cache\"\"\"\n",
    "\n",
    "    REQUIRED_PACKAGES = {\n",
    "        'core': [\n",
    "            'gspread', 'google-auth', 'google-colab',\n",
    "            'pandas', 'numpy', 'matplotlib', 'seaborn', 'scipy'\n",
    "        ],\n",
    "        'financial': [\n",
    "            'yfinance',           # Dados de mercado\n",
    "            'pandas-ta',          # Indicadores t√©cnicos\n",
    "            'quantstats',         # Relat√≥rios profissionais\n",
    "        ],\n",
    "        'visualization': [\n",
    "            'plotly',            # Gr√°ficos interativos\n",
    "            'kaleido'           # Export de gr√°ficos\n",
    "        ],\n",
    "        'performance': [\n",
    "            'numba',            # Compila√ß√£o JIT\n",
    "            'joblib'            # Paraleliza√ß√£o\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def install_packages(cls, categories: List[str] = None):\n",
    "        \"\"\"Instala√ß√£o inteligente com verifica√ß√£o de cache\"\"\"\n",
    "        if categories is None:\n",
    "            categories = ['core', 'financial']\n",
    "\n",
    "        for category in categories:\n",
    "            packages = cls.REQUIRED_PACKAGES.get(category, [])\n",
    "            for package in packages:\n",
    "                cls._install_if_needed(package)\n",
    "\n",
    "    @staticmethod\n",
    "    def _install_if_needed(package: str):\n",
    "        \"\"\"Instala pacote apenas se necess√°rio\"\"\"\n",
    "        try:\n",
    "            module_name = package.replace('-', '_')\n",
    "            importlib.import_module(module_name)\n",
    "            print(f\"‚úì {package} j√° instalado\")\n",
    "        except ImportError:\n",
    "            print(f\"üì¶ Instalando {package}...\")\n",
    "            try:\n",
    "                subprocess.check_call([\n",
    "                    sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package\n",
    "                ])\n",
    "                print(f\"‚úì {package} instalado com sucesso\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Erro ao instalar {package}: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VALIDADOR DE DADOS FINANCEIROS\n",
    "# =============================================================================\n",
    "class DataValidator:\n",
    "    \"\"\"Valida√ß√£o e limpeza de dados para evitar vieses\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config, logger: FinancialLogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "\n",
    "    def validate_temporal_consistency(self, df: pd.DataFrame) -> List[Dict]:\n",
    "        \"\"\"Verifica consist√™ncia temporal dos dados\"\"\"\n",
    "        issues = []\n",
    "\n",
    "        for empresa in df['EMPRESA'].unique():\n",
    "            empresa_df = df[df['EMPRESA'] == empresa].sort_values('DATA')\n",
    "            date_diff = empresa_df['DATA'].diff()\n",
    "\n",
    "            # Detectar gaps temporais\n",
    "            large_gaps = date_diff[date_diff > pd.Timedelta(days=self.config.VALIDATION['max_gap_days'])]\n",
    "            if not large_gaps.empty:\n",
    "                issue = {\n",
    "                    'empresa': empresa,\n",
    "                    'tipo': 'gap_temporal',\n",
    "                    'datas': large_gaps.index.tolist(),\n",
    "                    'gaps_days': large_gaps.dt.days.tolist()\n",
    "                }\n",
    "                issues.append(issue)\n",
    "                self.logger.log_anomaly(empresa, 'Temporal gap detected', issue)\n",
    "\n",
    "        return issues\n",
    "\n",
    "    def detect_outliers(self, df: pd.DataFrame, columns: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Detecta e marca outliers usando Z-score\"\"\"\n",
    "        if columns is None:\n",
    "            columns = list(self.config.SCCS_WEIGHTS.keys())\n",
    "\n",
    "        outlier_mask = pd.DataFrame(False, index=df.index, columns=columns)\n",
    "\n",
    "        for col in columns:\n",
    "            if col in df.columns:\n",
    "                std = df[col].std()\n",
    "                if pd.isna(std) or np.isclose(std, 0):\n",
    "                    outlier_mask[col] = False\n",
    "                    continue\n",
    "\n",
    "                mean = df[col].mean()\n",
    "                z_scores = np.abs((df[col] - mean) / std)\n",
    "                outlier_mask[col] = z_scores > self.config.VALIDATION['outlier_std_threshold']\n",
    "\n",
    "                n_outliers = outlier_mask[col].sum()\n",
    "                if n_outliers > 0:\n",
    "                    self.logger.log_anomaly('GLOBAL', f'{n_outliers} outliers in {col}', None)\n",
    "\n",
    "        return outlier_mask\n",
    "\n",
    "    def prevent_look_ahead_bias(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Previne look-ahead bias validando dados temporais\"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        for empresa in df['EMPRESA'].unique():\n",
    "            mask = df['EMPRESA'] == empresa\n",
    "            empresa_df = df[mask].sort_values('DATA')\n",
    "\n",
    "            # Verificar consist√™ncia temporal dos dados\n",
    "            for i in range(1, len(empresa_df)):\n",
    "                current_date = empresa_df.iloc[i]['DATA']\n",
    "                previous_date = empresa_df.iloc[i-1]['DATA']\n",
    "\n",
    "                # Verificar se h√° gaps muito grandes entre dados consecutivos\n",
    "                days_diff = (current_date - previous_date).days\n",
    "\n",
    "                # Se o gap for muito grande (> 6 meses), marcar como suspeito\n",
    "                if days_diff > 180:\n",
    "                    self.logger.log_anomaly(\n",
    "                        empresa,\n",
    "                        f'Large temporal gap detected',\n",
    "                        {'current': current_date, 'previous': previous_date, 'days': days_diff}\n",
    "                    )\n",
    "                    # Opcionalmente, marcar indicadores de crescimento como NaN\n",
    "                    # se o gap for muito grande\n",
    "                    if 'SALES_GROWTH' in df.columns and days_diff > 365:\n",
    "                        df.loc[empresa_df.index[i], 'SALES_GROWTH'] = np.nan\n",
    "\n",
    "        return df\n",
    "\n",
    "    def validate_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Relat√≥rio completo de qualidade dos dados\"\"\"\n",
    "        report = {\n",
    "            'total_records': len(df),\n",
    "            'unique_companies': df['EMPRESA'].nunique(),\n",
    "            'date_range': {\n",
    "                'start': df['DATA'].min(),\n",
    "                'end': df['DATA'].max()\n",
    "            },\n",
    "            'missing_data': {},\n",
    "            'outliers': {},\n",
    "            'temporal_issues': []\n",
    "        }\n",
    "\n",
    "        # An√°lise de dados faltantes\n",
    "        for col in self.config.SCCS_WEIGHTS.keys():\n",
    "            if col in df.columns:\n",
    "                missing_pct = (df[col].isna().sum() / len(df)) * 100\n",
    "                report['missing_data'][col] = f\"{missing_pct:.2f}%\"\n",
    "\n",
    "        # Detec√ß√£o de outliers\n",
    "        outlier_mask = self.detect_outliers(df)\n",
    "        for col in outlier_mask.columns:\n",
    "            report['outliers'][col] = outlier_mask[col].sum()\n",
    "\n",
    "        # Issues temporais\n",
    "        report['temporal_issues'] = self.validate_temporal_consistency(df)\n",
    "\n",
    "        return report\n",
    "\n",
    "# =============================================================================\n",
    "# CALCULADOR SCCS OTIMIZADO\n",
    "# =============================================================================\n",
    "class OptimizedSCCSCalculator:\n",
    "    \"\"\"Calculador SCCS otimizado com vetoriza√ß√£o e cache\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config, logger: FinancialLogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.cache = {}\n",
    "\n",
    "    def get_cached_or_compute(self, key: str, compute_func, *args, **kwargs):\n",
    "        \"\"\"Cache com persist√™ncia em disco\"\"\"\n",
    "        # Criar diret√≥rio de cache\n",
    "        cache_dir = 'sccs_cache'\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "        # Gerar hash √∫nico\n",
    "        cache_key = hashlib.md5(\n",
    "            f\"{key}_{str(args)}_{str(kwargs)}\".encode()\n",
    "        ).hexdigest()\n",
    "\n",
    "        cache_file = f\"{cache_dir}/{cache_key}.pkl\"\n",
    "\n",
    "        # Verificar cache\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    self.logger.logger.info(f\"Cache hit para {key}\")\n",
    "                    return pickle.load(f)\n",
    "            except:\n",
    "                pass  # Cache corrompido, recalcular\n",
    "\n",
    "        # Calcular e salvar\n",
    "        result = compute_func(*args, **kwargs)\n",
    "\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(result, f)\n",
    "        except:\n",
    "            pass  # Falha ao salvar cache n√£o √© cr√≠tica\n",
    "\n",
    "        return result\n",
    "\n",
    "    def calculate_sccs_parallel(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Calcula SCCS em paralelo se poss√≠vel\"\"\"\n",
    "        try:\n",
    "            from joblib import Parallel, delayed\n",
    "\n",
    "            # Processar em paralelo por empresa\n",
    "            def process_empresa(empresa_df):\n",
    "                return self.calculate_sccs_vectorized(empresa_df)\n",
    "\n",
    "            results = Parallel(n_jobs=-1, backend='threading')(\n",
    "                delayed(process_empresa)(group)\n",
    "                for _, group in df.groupby('EMPRESA')\n",
    "            )\n",
    "\n",
    "            return np.concatenate(results)\n",
    "\n",
    "        except ImportError:\n",
    "            # Fallback para vers√£o serial\n",
    "            self.logger.logger.info(\"joblib n√£o dispon√≠vel, usando processamento serial\")\n",
    "            return self.calculate_sccs_vectorized(df)\n",
    "\n",
    "    def calculate_sccs_vectorized(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Calcula SCCS para todo DataFrame de uma vez usando vetoriza√ß√£o\"\"\"\n",
    "        scores_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "        # Calcular scores para cada indicador\n",
    "        for indicator, weight in self.config.SCCS_WEIGHTS.items():\n",
    "            if indicator in df.columns:\n",
    "                threshold = self.config.THRESHOLDS.get(indicator, {})\n",
    "\n",
    "                if indicator == 'RETURN_COM_EQY':\n",
    "                    scores_df[indicator] = self._score_return_com_eqy_vectorized(df[indicator])\n",
    "                elif indicator == 'CF_FREE_CASH_FLOW':\n",
    "                    scores_df[indicator] = self._score_fcf_vectorized(df, indicator)\n",
    "                else:\n",
    "                    scores_df[indicator] = self._score_vectorized(\n",
    "                        df[indicator].values,\n",
    "                        threshold.get('neutral', 0),\n",
    "                        threshold.get('worst', -10),\n",
    "                        threshold.get('best', 10)\n",
    "                    )\n",
    "\n",
    "        # Calcular SCCS ponderado\n",
    "        if scores_df.empty:\n",
    "            self.logger.logger.warning(\n",
    "                \"Nenhuma coluna de indicador dispon√≠vel para c√°lculo do SCCS; retornando zeros\"\n",
    "            )\n",
    "            return np.zeros(len(df), dtype=float)\n",
    "\n",
    "        weights = np.array([self.config.SCCS_WEIGHTS.get(col, 0) for col in scores_df.columns])\n",
    "        weight_sum = weights.sum()\n",
    "\n",
    "        if weight_sum <= 0 or not np.isfinite(weight_sum):\n",
    "            normalized_weights = np.zeros_like(weights)\n",
    "        else:\n",
    "            normalized_weights = weights / weight_sum  # Normalizar pesos\n",
    "\n",
    "        sccs_scores = (scores_df.values @ normalized_weights) * 5\n",
    "\n",
    "        # Log de algumas m√©tricas para auditoria\n",
    "        for empresa in df['EMPRESA'].unique()[:5]:  # Log primeiras 5 empresas\n",
    "            empresa_mask = df['EMPRESA'] == empresa\n",
    "            if empresa_mask.any():\n",
    "                idx = empresa_mask.idxmax()\n",
    "                self.logger.log_calculation(\n",
    "                    empresa,\n",
    "                    'SCCS',\n",
    "                    sccs_scores[idx],\n",
    "                    {'scores': scores_df.loc[idx].to_dict()}\n",
    "                )\n",
    "\n",
    "        return sccs_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def _score_vectorized(values: np.ndarray, neutral: float, worst: float, best: float) -> np.ndarray:\n",
    "        \"\"\"Fun√ß√£o de scoring vetorizada\"\"\"\n",
    "        scores = np.zeros_like(values, dtype=float)\n",
    "\n",
    "        # Tratar NaN\n",
    "        valid_mask = ~np.isnan(values)\n",
    "        valid_values = values[valid_mask]\n",
    "\n",
    "        # Aplicar scoring\n",
    "        scores[valid_mask] = np.where(\n",
    "            valid_values >= best, 10,\n",
    "            np.where(\n",
    "                valid_values <= worst, -10,\n",
    "                np.where(\n",
    "                    valid_values >= neutral,\n",
    "                    10 * (valid_values - neutral) / (best - neutral),\n",
    "                    -10 * (neutral - valid_values) / (neutral - worst)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def _score_return_com_eqy_vectorized(self, values: pd.Series) -> np.ndarray:\n",
    "        \"\"\"Score ROE usando tangente hiperb√≥lica\"\"\"\n",
    "        threshold = self.config.THRESHOLDS['RETURN_COM_EQY']\n",
    "        return np.clip(\n",
    "            np.tanh((values - threshold['neutral']) / threshold['scale']) * 10,\n",
    "            -10, 10\n",
    "        )\n",
    "\n",
    "    def _score_fcf_vectorized(self, df: pd.DataFrame, indicator: str) -> np.ndarray:\n",
    "        \"\"\"Score FCF com an√°lise hist√≥rica - CORRIGIDO COM PROTE√á√ÉO CONTRA DIVIS√ÉO POR ZERO\"\"\"\n",
    "        scores = np.zeros(len(df))\n",
    "\n",
    "        for empresa in df['EMPRESA'].unique():\n",
    "            mask = df['EMPRESA'] == empresa\n",
    "            empresa_df = df[mask]\n",
    "\n",
    "            # Se REVENUE existir, usar para calcular margem FCF\n",
    "            if 'REVENUE' in df.columns:\n",
    "                # CORRE√á√ÉO CR√çTICA: Prote√ß√£o contra divis√£o por zero\n",
    "                revenue_safe = empresa_df['REVENUE'].replace(0, np.nan)\n",
    "                fcf_margin = (empresa_df[indicator] / revenue_safe) * 100\n",
    "\n",
    "                scores[mask] = self._score_vectorized(\n",
    "                    fcf_margin.values,\n",
    "                    self.config.THRESHOLDS['CF_FREE_CASH_FLOW']['neutral'],\n",
    "                    self.config.THRESHOLDS['CF_FREE_CASH_FLOW']['worst'],\n",
    "                    self.config.THRESHOLDS['CF_FREE_CASH_FLOW']['best']\n",
    "                )\n",
    "            else:\n",
    "                # Caso contr√°rio, usar o valor absoluto do FCF\n",
    "                scores[mask] = self._score_vectorized(\n",
    "                    empresa_df[indicator].values,\n",
    "                    self.config.THRESHOLDS['CF_FREE_CASH_FLOW']['neutral'],\n",
    "                    self.config.THRESHOLDS['CF_FREE_CASH_FLOW']['worst'],\n",
    "                    self.config.THRESHOLDS['CF_FREE_CASH_FLOW']['best']\n",
    "                )\n",
    "\n",
    "        return scores\n",
    "\n",
    "# =============================================================================\n",
    "# M√âTRICAS FINANCEIRAS AVAN√áADAS\n",
    "# =============================================================================\n",
    "class FinancialMetrics:\n",
    "    \"\"\"C√°lculo de m√©tricas financeiras profissionais\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config, logger: FinancialLogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "\n",
    "    def calculate_risk_metrics(self, returns: pd.Series) -> Dict[str, float]:\n",
    "        \"\"\"Calcula m√©tricas de risco padr√£o do mercado\"\"\"\n",
    "        try:\n",
    "            metrics = {\n",
    "                'mean_return': returns.mean(),\n",
    "                'volatility': returns.std(),\n",
    "                'sharpe_ratio': self._sharpe_ratio(returns),\n",
    "                'sortino_ratio': self._sortino_ratio(returns),\n",
    "                'max_drawdown': self._max_drawdown(returns),\n",
    "                'var_95': returns.quantile(0.05),\n",
    "                'cvar_95': returns[returns <= returns.quantile(0.05)].mean(),\n",
    "                'skewness': returns.skew(),\n",
    "                'kurtosis': returns.kurtosis()\n",
    "            }\n",
    "\n",
    "            self.logger.log_calculation('PORTFOLIO', 'Risk Metrics', 0, metrics)\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error('calculate_risk_metrics', e)\n",
    "            return {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _sharpe_ratio(returns: pd.Series, risk_free: float = 0.02) -> float:\n",
    "        \"\"\"Calcula Sharpe Ratio\"\"\"\n",
    "        excess_returns = returns - risk_free/252\n",
    "        return np.sqrt(252) * excess_returns.mean() / excess_returns.std()\n",
    "\n",
    "    @staticmethod\n",
    "    def _sortino_ratio(returns: pd.Series, risk_free: float = 0.02) -> float:\n",
    "        \"\"\"Calcula Sortino Ratio\"\"\"\n",
    "        excess_returns = returns - risk_free/252\n",
    "        downside_returns = excess_returns[excess_returns < 0]\n",
    "        downside_std = downside_returns.std()\n",
    "        return np.sqrt(252) * excess_returns.mean() / downside_std if downside_std > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def _max_drawdown(returns: pd.Series) -> float:\n",
    "        \"\"\"Calcula Maximum Drawdown\"\"\"\n",
    "        cumulative = (1 + returns).cumprod()\n",
    "        running_max = cumulative.cummax()\n",
    "        drawdown = (cumulative - running_max) / running_max\n",
    "        return drawdown.min()\n",
    "\n",
    "    def regime_detection(self, data: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Detecta mudan√ßas de regime no mercado\"\"\"\n",
    "        try:\n",
    "            from sklearn.mixture import GaussianMixture\n",
    "\n",
    "            # Preparar features\n",
    "            features = data[['SCCS', 'DIRECAO', 'ACELERACAO']].fillna(0).values\n",
    "\n",
    "            # Modelo de mistura gaussiana\n",
    "            gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "            regimes = gmm.fit_predict(features)\n",
    "\n",
    "            # Mapear regimes baseado nas m√©dias\n",
    "            means = gmm.means_[:, 0]  # Usar SCCS como refer√™ncia\n",
    "            sorted_idx = np.argsort(means)\n",
    "            regime_map = {sorted_idx[0]: 'Bear', sorted_idx[1]: 'Neutral', sorted_idx[2]: 'Bull'}\n",
    "\n",
    "            return [regime_map[r] for r in regimes]\n",
    "\n",
    "        except ImportError:\n",
    "            self.logger.log_anomaly('GLOBAL', 'sklearn not available for regime detection', None)\n",
    "            return ['Unknown'] * len(data)\n",
    "\n",
    "# =============================================================================\n",
    "# NOVA FUNCIONALIDADE: PREVIS√ÉO DE CICLOS\n",
    "# =============================================================================\n",
    "class CyclePrediction:\n",
    "    \"\"\"Sistema de previs√£o do pr√≥ximo ciclo microecon√¥mico\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config, logger: FinancialLogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.transition_matrix = None\n",
    "        self.cycle_history = {}\n",
    "\n",
    "    def build_transition_matrix(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Constr√≥i matriz de transi√ß√£o de Markov\"\"\"\n",
    "        cycles = ['Expans√£o', 'Retra√ß√£o', 'Desacelera√ß√£o', 'Recupera√ß√£o']\n",
    "        matrix = np.zeros((4, 4))\n",
    "\n",
    "        for empresa in df['EMPRESA'].unique():\n",
    "            empresa_df = df[df['EMPRESA'] == empresa].sort_values('DATA')\n",
    "\n",
    "            # Guardar hist√≥rico\n",
    "            self.cycle_history[empresa] = empresa_df['CICLO'].tolist()\n",
    "\n",
    "            # Calcular transi√ß√µes\n",
    "            for i in range(len(empresa_df) - 1):\n",
    "                current = empresa_df.iloc[i]['CICLO']\n",
    "                next_cycle = empresa_df.iloc[i + 1]['CICLO']\n",
    "\n",
    "                if current in cycles and next_cycle in cycles:\n",
    "                    curr_idx = cycles.index(current)\n",
    "                    next_idx = cycles.index(next_cycle)\n",
    "                    matrix[curr_idx, next_idx] += 1\n",
    "\n",
    "        # Normalizar\n",
    "        row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "        matrix = np.divide(matrix, row_sums, where=row_sums != 0)\n",
    "\n",
    "        self.transition_matrix = matrix\n",
    "        self.logger.log_calculation('SYSTEM', 'Transition Matrix Built', 0,\n",
    "                                   {'shape': matrix.shape})\n",
    "        return matrix\n",
    "\n",
    "    def predict_next_cycle(self, empresa: str, current_data: pd.Series) -> Dict:\n",
    "        \"\"\"Prev√™ pr√≥ximo ciclo com confian√ßa\"\"\"\n",
    "        cycles = ['Expans√£o', 'Retra√ß√£o', 'Desacelera√ß√£o', 'Recupera√ß√£o']\n",
    "        current_cycle = current_data.get('CICLO', 'Indefinido')\n",
    "\n",
    "        # Probabilidades base (Markov)\n",
    "        if self.transition_matrix is not None and current_cycle in cycles:\n",
    "            curr_idx = cycles.index(current_cycle)\n",
    "            base_probs = self.transition_matrix[curr_idx]\n",
    "        else:\n",
    "            base_probs = np.ones(4) / 4\n",
    "\n",
    "        # Ajuste por indicadores\n",
    "        indicator_probs = self._analyze_indicators(current_data)\n",
    "\n",
    "        # Ajuste por padr√£o hist√≥rico\n",
    "        pattern_probs = self._analyze_pattern(empresa)\n",
    "\n",
    "        # Combina√ß√£o ponderada\n",
    "        final_probs = (base_probs * 0.5 +\n",
    "                      indicator_probs * 0.3 +\n",
    "                      pattern_probs * 0.2)\n",
    "\n",
    "        # Normalizar\n",
    "        final_probs = final_probs / final_probs.sum()\n",
    "\n",
    "        # Resultado\n",
    "        predicted_idx = np.argmax(final_probs)\n",
    "\n",
    "        return {\n",
    "            'current_cycle': current_cycle,\n",
    "            'predicted_cycle': cycles[predicted_idx],\n",
    "            'confidence': float(final_probs[predicted_idx]),\n",
    "            'probabilities': dict(zip(cycles, final_probs.tolist())),\n",
    "            'indicators': {\n",
    "                'SCCS': current_data.get('SCCS', 0),\n",
    "                'SALES_GROWTH': current_data.get('SALES_GROWTH', 0),\n",
    "                'ROE': current_data.get('RETURN_COM_EQY', 0),\n",
    "                'MARGIN': current_data.get('OPER_MARGIN', 0)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _analyze_indicators(self, data: pd.Series) -> np.ndarray:\n",
    "        \"\"\"An√°lise baseada em indicadores\"\"\"\n",
    "        probs = np.zeros(4)  # [Expans√£o, Retra√ß√£o, Desacelera√ß√£o, Recupera√ß√£o]\n",
    "\n",
    "        sales = data.get('SALES_GROWTH', 0)\n",
    "        roe = data.get('RETURN_COM_EQY', 0)\n",
    "        margin = data.get('OPER_MARGIN', 0)\n",
    "        sccs = data.get('SCCS', 0)\n",
    "\n",
    "        # L√≥gica de probabilidades\n",
    "        if sales > 20 and roe > 15:\n",
    "            probs[0] = 0.6  # Expans√£o\n",
    "        elif sales > 5 and sales < 15:\n",
    "            probs[1] = 0.5  # Retra√ß√£o\n",
    "        elif sales < 0 or roe < 5:\n",
    "            probs[2] = 0.6  # Desacelera√ß√£o\n",
    "        elif sales > 0 and sccs > 0:\n",
    "            probs[3] = 0.5  # Recupera√ß√£o\n",
    "\n",
    "        # Normalizar\n",
    "        if probs.sum() == 0:\n",
    "            probs = np.ones(4) / 4\n",
    "        else:\n",
    "            probs = probs / probs.sum()\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def _analyze_pattern(self, empresa: str) -> np.ndarray:\n",
    "        \"\"\"An√°lise de padr√µes hist√≥ricos\"\"\"\n",
    "        if empresa not in self.cycle_history:\n",
    "            return np.ones(4) / 4\n",
    "\n",
    "        history = self.cycle_history[empresa]\n",
    "        if len(history) < 3:\n",
    "            return np.ones(4) / 4\n",
    "\n",
    "        # Identificar padr√µes comuns\n",
    "        recent = history[-3:]\n",
    "        cycles = ['Expans√£o', 'Retra√ß√£o', 'Desacelera√ß√£o', 'Recupera√ß√£o']\n",
    "        probs = np.zeros(4)\n",
    "\n",
    "        # Padr√µes conhecidos\n",
    "        if recent == ['Desacelera√ß√£o', 'Desacelera√ß√£o', 'Recupera√ß√£o']:\n",
    "            probs[0] = 0.6  # Prov√°vel Expans√£o\n",
    "        elif recent == ['Expans√£o', 'Expans√£o', 'Retra√ß√£o']:\n",
    "            probs[2] = 0.5  # Poss√≠vel Desacelera√ß√£o\n",
    "\n",
    "        # Normalizar\n",
    "        if probs.sum() == 0:\n",
    "            probs = np.ones(4) / 4\n",
    "        else:\n",
    "            probs = probs / probs.sum()\n",
    "\n",
    "        return probs\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZADOR INTERATIVO\n",
    "# =============================================================================\n",
    "class InteractiveVisualizer:\n",
    "    \"\"\"Cria√ß√£o de visualiza√ß√µes interativas com Plotly\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config, logger: FinancialLogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "\n",
    "    def create_interactive_dashboard(self, results_df: pd.DataFrame, metrics: Dict) -> Any:\n",
    "        \"\"\"Dashboard interativo para an√°lise\"\"\"\n",
    "        try:\n",
    "            import plotly.graph_objects as go\n",
    "            from plotly.subplots import make_subplots\n",
    "\n",
    "            fig = make_subplots(\n",
    "                rows=3, cols=2,\n",
    "                subplot_titles=(\n",
    "                    'SCCS Evolution', 'Cycle Distribution',\n",
    "                    'Performance Heatmap', 'Risk Metrics',\n",
    "                    'Correlation Matrix', 'Regime Analysis'\n",
    "                ),\n",
    "                specs=[\n",
    "                    [{'type': 'scatter'}, {'type': 'pie'}],\n",
    "                    [{'type': 'heatmap'}, {'type': 'bar'}],\n",
    "                    [{'type': 'heatmap'}, {'type': 'scatter'}]\n",
    "                ],\n",
    "                vertical_spacing=0.1,\n",
    "                horizontal_spacing=0.15\n",
    "            )\n",
    "\n",
    "            # 1. SCCS Evolution\n",
    "            for empresa in results_df['EMPRESA'].unique()[:10]:  # Top 10 empresas\n",
    "                empresa_data = results_df[results_df['EMPRESA'] == empresa]\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=empresa_data['DATA'],\n",
    "                        y=empresa_data['SCCS'],\n",
    "                        name=empresa,\n",
    "                        mode='lines+markers',\n",
    "                        hovertemplate='<b>%{text}</b><br>SCCS: %{y:.2f}<br>Data: %{x}',\n",
    "                        text=empresa_data['EMPRESA']\n",
    "                    ),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "\n",
    "            # 2. Cycle Distribution\n",
    "            cycle_counts = results_df['CICLO'].value_counts()\n",
    "            fig.add_trace(\n",
    "                go.Pie(\n",
    "                    labels=cycle_counts.index,\n",
    "                    values=cycle_counts.values,\n",
    "                    hole=0.3,\n",
    "                    marker=dict(colors=['#27ae60', '#c0392b', '#7f8c8d', '#2c3e50'])\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "\n",
    "            # 3. Performance Heatmap\n",
    "            pivot_data = results_df.pivot_table(\n",
    "                index='EMPRESA',\n",
    "                columns='TRIMESTRE_STR',\n",
    "                values='SCCS',\n",
    "                aggfunc='mean'\n",
    "            ).iloc[:15, -8:]  # Top 15 empresas, √∫ltimos 8 trimestres\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Heatmap(\n",
    "                    z=pivot_data.values,\n",
    "                    x=pivot_data.columns,\n",
    "                    y=pivot_data.index,\n",
    "                    colorscale='RdYlGn',\n",
    "                    zmid=0\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "\n",
    "            # 4. Risk Metrics Bar Chart\n",
    "            if metrics:\n",
    "                fig.add_trace(\n",
    "                    go.Bar(\n",
    "                        x=list(metrics.keys()),\n",
    "                        y=list(metrics.values()),\n",
    "                        marker_color='lightblue'\n",
    "                    ),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "\n",
    "            # 5. Correlation Matrix\n",
    "            indicators = [col for col in results_df.columns if col in self.config.SCCS_WEIGHTS.keys()]\n",
    "            if indicators:\n",
    "                corr_matrix = results_df[indicators].corr()\n",
    "                fig.add_trace(\n",
    "                    go.Heatmap(\n",
    "                        z=corr_matrix.values,\n",
    "                        x=corr_matrix.columns,\n",
    "                        y=corr_matrix.index,\n",
    "                        colorscale='Viridis'\n",
    "                    ),\n",
    "                    row=3, col=1\n",
    "                )\n",
    "\n",
    "            # 6. Regime Analysis\n",
    "            if 'REGIME' in results_df.columns:\n",
    "                regime_data = results_df.groupby(['DATA', 'REGIME']).size().unstack(fill_value=0)\n",
    "                for regime in regime_data.columns:\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=regime_data.index,\n",
    "                            y=regime_data[regime],\n",
    "                            name=regime,\n",
    "                            stackgroup='one'\n",
    "                        ),\n",
    "                        row=3, col=2\n",
    "                    )\n",
    "\n",
    "            # Configurar layout\n",
    "            fig.update_layout(\n",
    "                height=1200,\n",
    "                showlegend=True,\n",
    "                hovermode='x unified',\n",
    "                template='plotly_dark',\n",
    "                title_text=\"SCCS Analysis Dashboard - Professional Edition\",\n",
    "                title_font_size=20\n",
    "            )\n",
    "\n",
    "            return fig\n",
    "\n",
    "        except ImportError:\n",
    "            self.logger.log_error('Plotly not available', Exception('Install plotly for interactive charts'))\n",
    "            return None\n",
    "\n",
    "    def create_cycle_plot_enhanced(self, empresa: str, results_df: pd.DataFrame):\n",
    "        \"\"\"Vers√£o melhorada do gr√°fico de ciclo - COM CORRE√á√ÉO DE MATPLOTLIB STYLE\"\"\"\n",
    "        empresa_df = results_df[results_df['EMPRESA'] == empresa].tail(\n",
    "            self.config.PLOT_CONFIG['lookback_periods']\n",
    "        )\n",
    "\n",
    "        if len(empresa_df) < self.config.PLOT_CONFIG['min_data_points']:\n",
    "            self.logger.log_anomaly(empresa, 'Insufficient data for cycle plot', len(empresa_df))\n",
    "            return\n",
    "\n",
    "        # CORRE√á√ÉO: Validar e aplicar style matplotlib\n",
    "        available_styles = plt.style.available\n",
    "        if 'seaborn-v0_8-darkgrid' in available_styles:\n",
    "            plt.style.use('seaborn-v0_8-darkgrid')\n",
    "        elif 'seaborn' in available_styles:\n",
    "            plt.style.use('seaborn')\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=self.config.PLOT_CONFIG['figure_size'])\n",
    "\n",
    "        # Configurar limites din√¢micos\n",
    "        max_abs_x = empresa_df['DIRECAO'].abs().max()\n",
    "        max_abs_y = empresa_df['ACELERACAO'].abs().max()\n",
    "        plot_limit = max(max_abs_x, max_abs_y, 5) * 1.2\n",
    "\n",
    "        ax.set_xlim(-plot_limit, plot_limit)\n",
    "        ax.set_ylim(-plot_limit, plot_limit)\n",
    "\n",
    "        # √Åreas dos quadrantes com preenchimento s√≥lido\n",
    "        from matplotlib.patches import Rectangle\n",
    "        from matplotlib.collections import PatchCollection\n",
    "\n",
    "        # Configurar preenchimento para cada quadrante\n",
    "        quadrants = {\n",
    "            'Expans√£o': {'xy': (0, 0), 'color': '#27ae60', 'alpha': 0.3},\n",
    "            'Retra√ß√£o': {'xy': (0, -plot_limit), 'color': '#c0392b', 'alpha': 0.3},\n",
    "            'Desacelera√ß√£o': {'xy': (-plot_limit, -plot_limit), 'color': '#7f8c8d', 'alpha': 0.3},\n",
    "            'Recupera√ß√£o': {'xy': (-plot_limit, 0), 'color': '#2c3e50', 'alpha': 0.3}\n",
    "        }\n",
    "\n",
    "        for quad_name, props in quadrants.items():\n",
    "            rect = Rectangle(\n",
    "                props['xy'],\n",
    "                plot_limit if props['xy'][0] >= 0 else plot_limit,\n",
    "                plot_limit if props['xy'][1] >= 0 else plot_limit,\n",
    "                facecolor=props['color'],\n",
    "                alpha=props['alpha'],\n",
    "                label=quad_name\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        # Spline suavizado com mais pontos\n",
    "        x = empresa_df['DIRECAO'].values\n",
    "        y = empresa_df['ACELERACAO'].values\n",
    "        t = np.arange(len(x))\n",
    "\n",
    "        if len(x) >= 4:  # Precisa de pelo menos 4 pontos para spline c√∫bico\n",
    "            spline_x = CubicSpline(t, x)\n",
    "            spline_y = CubicSpline(t, y)\n",
    "            t_smooth = np.linspace(t.min(), t.max(), 500)\n",
    "            x_smooth = spline_x(t_smooth)\n",
    "            y_smooth = spline_y(t_smooth)\n",
    "\n",
    "            # Linha com gradiente de cor\n",
    "            points = np.array([x_smooth, y_smooth]).T.reshape(-1, 1, 2)\n",
    "            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "\n",
    "            from matplotlib.collections import LineCollection\n",
    "            lc = LineCollection(segments, cmap='viridis', linewidth=3)\n",
    "            lc.set_array(t_smooth)\n",
    "            ax.add_collection(lc)\n",
    "        else:\n",
    "            ax.plot(x, y, color='#00aaff', linewidth=2.5)\n",
    "\n",
    "        # Pontos com tamanhos vari√°veis baseados em SCCS\n",
    "        sccs_normalized = (empresa_df['SCCS'] - empresa_df['SCCS'].min()) / (empresa_df['SCCS'].max() - empresa_df['SCCS'].min() + 0.001)\n",
    "        sizes = 50 + sccs_normalized * 150\n",
    "\n",
    "        scatter = ax.scatter(x, y, c=empresa_df['SCCS'], s=sizes,\n",
    "                           cmap='RdYlGn', edgecolors='white', linewidth=2,\n",
    "                           alpha=0.8, zorder=5)\n",
    "\n",
    "        # Colorbar para SCCS\n",
    "        cbar = plt.colorbar(scatter, ax=ax, pad=0.02)\n",
    "        cbar.set_label('SCCS Score', rotation=270, labelpad=15, color='white')\n",
    "        cbar.ax.yaxis.set_tick_params(color='white')\n",
    "        plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color='white')\n",
    "\n",
    "        # Anota√ß√µes melhoradas\n",
    "        for i, row in empresa_df.iterrows():\n",
    "            # Apenas mostrar labels para pontos importantes\n",
    "            if i == empresa_df.index[0] or i == empresa_df.index[-1] or abs(row['SCCS']) > empresa_df['SCCS'].quantile(0.9):\n",
    "                ax.annotate(\n",
    "                    row['TRIMESTRE_STR'],\n",
    "                    xy=(row['DIRECAO'], row['ACELERACAO']),\n",
    "                    xytext=(5, 5),\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=8,\n",
    "                    color='white',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='purple', alpha=0.7),\n",
    "                    arrowprops=dict(arrowstyle='->', color='white', lw=0.5)\n",
    "                )\n",
    "\n",
    "        # Posi√ß√£o atual com destaque\n",
    "        last_point = empresa_df.iloc[-1]\n",
    "        ax.scatter(last_point['DIRECAO'], last_point['ACELERACAO'],\n",
    "                  color='yellow', s=300, marker='*', edgecolors='red',\n",
    "                  linewidth=2, zorder=10, label='Posi√ß√£o Atual')\n",
    "\n",
    "        # Grid e estilo\n",
    "        ax.grid(True, alpha=0.2, linestyle='--', linewidth=0.5)\n",
    "        ax.axhline(0, color='white', lw=1, linestyle='-', alpha=0.5)\n",
    "        ax.axvline(0, color='white', lw=1, linestyle='-', alpha=0.5)\n",
    "\n",
    "        # T√≠tulos e labels\n",
    "        ax.set_title(f'Ciclo Fundamentalista Avan√ßado - {empresa}',\n",
    "                    fontsize=18, color='white', fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Dire√ß√£o (N√≠vel dos Fundamentos)', fontsize=12, color='white')\n",
    "        ax.set_ylabel('Acelera√ß√£o (Momentum dos Fundamentos)', fontsize=12, color='white')\n",
    "\n",
    "        # Estilo dark theme\n",
    "        ax.set_facecolor('#0a0a0a')\n",
    "        fig.set_facecolor('#0a0a0a')\n",
    "        ax.tick_params(colors='white')\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color('white')\n",
    "            spine.set_linewidth(0.5)\n",
    "\n",
    "        # Labels dos quadrantes\n",
    "        quadrant_labels = {\n",
    "            'EXPANS√ÉO': (plot_limit*0.5, plot_limit*0.9),\n",
    "            'RETRA√á√ÉO': (plot_limit*0.5, -plot_limit*0.9),\n",
    "            'DESACELERA√á√ÉO': (-plot_limit*0.5, -plot_limit*0.9),\n",
    "            'RECUPERA√á√ÉO': (-plot_limit*0.5, plot_limit*0.9)\n",
    "        }\n",
    "\n",
    "        for label, (x_pos, y_pos) in quadrant_labels.items():\n",
    "            ax.text(x_pos, y_pos, label, ha='center', va='center',\n",
    "                   fontsize=14, color='white', fontweight='bold',\n",
    "                   bbox=dict(boxstyle='round,pad=0.5', facecolor='black', alpha=0.5))\n",
    "\n",
    "        # Legenda\n",
    "        ax.legend(loc='upper left', framealpha=0.9, facecolor='black',\n",
    "                 edgecolor='white', labelcolor='white')\n",
    "\n",
    "        # Informa√ß√µes adicionais\n",
    "        info_text = f\"SCCS Atual: {last_point['SCCS']:.2f}\\n\"\n",
    "        info_text += f\"Ciclo: {last_point['CICLO']}\\n\"\n",
    "        info_text += f\"Tend√™ncia: {'‚Üë' if last_point['DIRECAO'] > 0 else '‚Üì'}\"\n",
    "\n",
    "        ax.text(0.02, 0.02, info_text, transform=ax.transAxes,\n",
    "               fontsize=10, color='white', verticalalignment='bottom',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', facecolor='black', alpha=0.7))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# PIPELINE PRINCIPAL REFATORADO\n",
    "# =============================================================================\n",
    "class SCCSAnalysisPipeline:\n",
    "    \"\"\"Pipeline principal com todas as melhorias implementadas\"\"\"\n",
    "\n",
    "    def __init__(self, sheet_url: str, config: Config = None):\n",
    "        self.config = config or Config()\n",
    "        self.config.SHEET_URL = sheet_url\n",
    "        self.logger = FinancialLogger(self.config)\n",
    "        self.validator = DataValidator(self.config, self.logger)\n",
    "        self.calculator = OptimizedSCCSCalculator(self.config, self.logger)\n",
    "        self.metrics = FinancialMetrics(self.config, self.logger)\n",
    "        self.visualizer = InteractiveVisualizer(self.config, self.logger)\n",
    "\n",
    "        self.df = None\n",
    "        self.results_df = pd.DataFrame()\n",
    "        self.risk_metrics = {}\n",
    "        self.dashboard = None\n",
    "        self.predictor = None  # Para o sistema de previs√£o\n",
    "        self.predictions = {}  # Para armazenar previs√µes\n",
    "\n",
    "    def validate_sheet_structure(self, all_values):\n",
    "        \"\"\"Valida estrutura antes de processar\"\"\"\n",
    "        if len(all_values) < 3:\n",
    "            raise ValueError(\"Planilha precisa ter pelo menos 3 linhas\")\n",
    "\n",
    "        # Verificar formato esperado\n",
    "        if not all_values[0] or not all_values[1]:\n",
    "            raise ValueError(\"Headers inv√°lidos\")\n",
    "\n",
    "        # Verificar se tem dados\n",
    "        if len(all_values[2:]) == 0:\n",
    "            raise ValueError(\"Planilha sem dados\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def cleanup_memory(self):\n",
    "        \"\"\"Libera mem√≥ria ap√≥s processamento pesado\"\"\"\n",
    "        # Limpar DataFrames tempor√°rios\n",
    "        temp_attrs = ['_temp_df', '_cache_df', '_intermediate_results']\n",
    "        for attr in temp_attrs:\n",
    "            if hasattr(self, attr):\n",
    "                delattr(self, attr)\n",
    "\n",
    "        # For√ßar garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        self.logger.logger.info(\"Mem√≥ria liberada com sucesso\")\n",
    "\n",
    "    def _create_gspread_client(self):\n",
    "        \"\"\"Cria cliente gspread com m√∫ltiplas estrat√©gias de autentica√ß√£o\"\"\"\n",
    "        service_file = self.config.GOOGLE_SERVICE_ACCOUNT_FILE\n",
    "        if service_file:\n",
    "            service_path = os.path.expanduser(service_file)\n",
    "            if not os.path.exists(service_path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Arquivo de credenciais n√£o encontrado: {service_path}\"\n",
    "                )\n",
    "            return gspread.service_account(filename=service_path)\n",
    "\n",
    "        env_credentials = os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "        if env_credentials:\n",
    "            env_path = os.path.expanduser(env_credentials)\n",
    "            if os.path.exists(env_path):\n",
    "                return gspread.service_account(filename=env_path)\n",
    "\n",
    "        try:\n",
    "            if COLAB_AUTH is not None:\n",
    "                COLAB_AUTH.authenticate_user()\n",
    "                creds, _ = default()\n",
    "                return gspread.authorize(creds)\n",
    "\n",
    "            creds, _ = default()\n",
    "            return gspread.authorize(creds)\n",
    "        except DefaultCredentialsError as exc:\n",
    "            raise RuntimeError(\n",
    "                \"Credenciais do Google n√£o encontradas. Defina GOOGLE_SERVICE_ACCOUNT_FILE na Config ou a vari√°vel de ambiente GOOGLE_APPLICATION_CREDENTIALS.\"\n",
    "            ) from exc\n",
    "\n",
    "    def load_and_prepare_data(self) -> bool:\n",
    "        \"\"\"Carrega e prepara dados com valida√ß√µes completas - VERS√ÉO CORRIGIDA\"\"\"\n",
    "        try:\n",
    "            self.logger.logger.info(\"=== INICIANDO CARREGAMENTO DE DADOS ===\")\n",
    "\n",
    "            if not self.config.SHEET_URL:\n",
    "                raise ValueError(\"URL da planilha n√£o configurada na Config\")\n",
    "\n",
    "            # AUTENTICA√á√ÉO SIMPLIFICADA E CORRETA\n",
    "            print(\"üîê Autenticando com Google Sheets...\")\n",
    "            gc_client = self._create_gspread_client()\n",
    "            print(\"‚úÖ Autentica√ß√£o conclu√≠da!\")\n",
    "\n",
    "            # Extrair sheet_id da URL\n",
    "            if '/d/' in self.config.SHEET_URL:\n",
    "                sheet_id = self.config.SHEET_URL.split('/d/')[1].split('/')[0]\n",
    "            else:\n",
    "                raise ValueError(\"URL da planilha inv√°lida\")\n",
    "\n",
    "            print(f\"üìÑ Acessando planilha ID: {sheet_id[:10]}...\")\n",
    "\n",
    "            # Abrir planilha\n",
    "            spreadsheet = gc_client.open_by_key(sheet_id)\n",
    "            worksheet_name = self.config.GOOGLE_WORKSHEET_NAME\n",
    "            if worksheet_name:\n",
    "                print(f\"üìÑ Utilizando aba configurada: {worksheet_name}\")\n",
    "                worksheet = spreadsheet.worksheet(worksheet_name)\n",
    "            else:\n",
    "                worksheet = spreadsheet.sheet1\n",
    "            all_values = worksheet.get_all_values()\n",
    "\n",
    "            print(\"‚úÖ Planilha acessada com sucesso!\")\n",
    "\n",
    "            # NOVA VALIDA√á√ÉO\n",
    "            self.validate_sheet_structure(all_values)\n",
    "\n",
    "            # Processar headers\n",
    "            header_empresas = all_values[0]\n",
    "            header_indicadores = all_values[1]\n",
    "            data_rows = all_values[2:]\n",
    "\n",
    "            print(f\"üìä Processando {len(data_rows)} linhas de dados...\")\n",
    "\n",
    "            dados_processados = []\n",
    "            coluna_map = {\n",
    "                i: (e.strip(), ind.strip())\n",
    "                for i, (e, ind) in enumerate(zip(header_empresas, header_indicadores))\n",
    "                if e and ind\n",
    "            }\n",
    "\n",
    "            # Processar linhas\n",
    "            for row_idx, row in enumerate(data_rows):\n",
    "                data_str = row[0] if len(row) > 0 else None\n",
    "                if not data_str:\n",
    "                    continue\n",
    "\n",
    "                dados_por_empresa = {}\n",
    "                for col_idx, (empresa, indicador) in coluna_map.items():\n",
    "                    if col_idx == 0:  # Pular coluna de data\n",
    "                        continue\n",
    "\n",
    "                    if empresa not in dados_por_empresa:\n",
    "                        dados_por_empresa[empresa] = {\n",
    "                            'EMPRESA': empresa,\n",
    "                            'DATA': data_str\n",
    "                        }\n",
    "\n",
    "                    valor_str = row[col_idx] if col_idx < len(row) else ''\n",
    "                    try:\n",
    "                        cleaned_val = str(valor_str).replace('%', '').strip()\n",
    "                        if cleaned_val in ('', '-', '#N/A', 'N/A', 'None'):\n",
    "                            valor = np.nan\n",
    "                        else:\n",
    "                            valor = float(cleaned_val.replace('.', '').replace(',', '.'))\n",
    "                    except (ValueError, TypeError) as e:\n",
    "                        valor = np.nan\n",
    "                        if row_idx < 5:  # Log apenas primeiras linhas\n",
    "                            self.logger.log_anomaly(\n",
    "                                empresa,\n",
    "                                f\"Error parsing {indicador}\",\n",
    "                                {'value': valor_str, 'error': str(e)}\n",
    "                            )\n",
    "\n",
    "                    dados_por_empresa[empresa][indicador] = valor\n",
    "\n",
    "                dados_processados.extend(dados_por_empresa.values())\n",
    "\n",
    "            # Criar DataFrame\n",
    "            df = pd.DataFrame(dados_processados)\n",
    "            if df.empty:\n",
    "                raise ValueError(\"Nenhum dado processado\")\n",
    "\n",
    "            # Processar datas\n",
    "            df['DATA'] = pd.to_datetime(df['DATA'], format='%d/%m/%Y', errors='coerce')\n",
    "            df.dropna(subset=['DATA'], inplace=True)\n",
    "\n",
    "            # Renomear colunas conforme necess√°rio\n",
    "            df.rename(columns={\n",
    "                'NET_DEBT_TO_SHRHLDR_EQTY': 'NET_DEBT_EQTY',\n",
    "                'CASH_FLOW_PER_SH': 'CF_FREE_CASH_FLOW',\n",
    "                'OPERATING_ROIC': 'OPERATING_ROIC'\n",
    "            }, inplace=True)\n",
    "\n",
    "            # Garantir que todas as colunas necess√°rias existam\n",
    "            for indicator in self.config.SCCS_WEIGHTS.keys():\n",
    "                if indicator not in df.columns:\n",
    "                    df[indicator] = np.nan\n",
    "                    self.logger.logger.warning(f\"Coluna {indicator} n√£o encontrada, preenchida com NaN\")\n",
    "\n",
    "            # Ordenar por empresa e data\n",
    "            df = df.sort_values(['EMPRESA', 'DATA']).reset_index(drop=True)\n",
    "\n",
    "            # CORRE√á√ÉO: SALES_GROWTH j√° vem da planilha, n√£o precisa calcular\n",
    "            if 'SALES_GROWTH' in df.columns:\n",
    "                df['SALES_GROWTH'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            else:\n",
    "                df['SALES_GROWTH'] = np.nan\n",
    "\n",
    "            # Valida√ß√µes\n",
    "            self.logger.logger.info(\"=== EXECUTANDO VALIDA√á√ïES ===\")\n",
    "\n",
    "            # 1. Detectar outliers\n",
    "            outliers = self.validator.detect_outliers(df)\n",
    "            self.logger.logger.info(f\"Outliers detectados: {outliers.sum().sum()}\")\n",
    "\n",
    "            # 2. Validar consist√™ncia temporal\n",
    "            temporal_issues = self.validator.validate_temporal_consistency(df)\n",
    "            self.logger.logger.info(f\"Issues temporais: {len(temporal_issues)}\")\n",
    "\n",
    "            # 3. Prevenir look-ahead bias\n",
    "            df = self.validator.prevent_look_ahead_bias(df)\n",
    "\n",
    "            # 4. Relat√≥rio de qualidade\n",
    "            quality_report = self.validator.validate_data_quality(df)\n",
    "            self.logger.logger.info(f\"Qualidade dos dados: {quality_report}\")\n",
    "\n",
    "            self.df = df\n",
    "            self.logger.logger.info(\n",
    "                f\"‚úÖ {len(df)} registros carregados de {df['EMPRESA'].nunique()} empresas\"\n",
    "            )\n",
    "\n",
    "            return True\n",
    "\n",
    "        except WorksheetNotFound as e:\n",
    "            self.logger.log_error('load_and_prepare_data', e)\n",
    "            print(f\"\\n‚ùå Aba configurada n√£o encontrada: {self.config.GOOGLE_WORKSHEET_NAME}\")\n",
    "            print(\"Verifique o nome da aba na planilha do Google Sheets.\")\n",
    "            return False\n",
    "\n",
    "        except SpreadsheetNotFound as e:\n",
    "            self.logger.log_error('load_and_prepare_data', e)\n",
    "            print(\"\\n‚ùå Planilha n√£o encontrada ou sem acesso.\")\n",
    "            print(\"Confirme o ID da planilha e as permiss√µes de compartilhamento.\")\n",
    "            return False\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error('load_and_prepare_data', e)\n",
    "            print(f\"\\n‚ùå Erro ao carregar dados: {str(e)}\")\n",
    "\n",
    "            # Mensagem de ajuda adicional\n",
    "            if \"403\" in str(e) or \"permission\" in str(e).lower():\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"‚ö†Ô∏è  ERRO: Acesso negado √† planilha.\")\n",
    "                print(\"=\"*80)\n",
    "                print(\"\\nPoss√≠veis solu√ß√µes:\")\n",
    "                print(\"\\n1. Verifique as permiss√µes da planilha:\")\n",
    "                print(\"   - Abra a planilha no Google Sheets\")\n",
    "                print(\"   - Clique em 'Compartilhar' (bot√£o verde no canto superior direito)\")\n",
    "                print(\"   - Configure para 'Qualquer pessoa com o link pode visualizar'\")\n",
    "                print(\"\\n2. Se a planilha √© privada:\")\n",
    "                print(\"   - Compartilhe com seu email do Google\")\n",
    "                print(\"   - Ou fa√ßa uma c√≥pia para sua conta\")\n",
    "                print(\"=\"*80)\n",
    "\n",
    "            return False\n",
    "\n",
    "    def run_analysis(self):\n",
    "        \"\"\"Executa an√°lise completa com SCCS, m√©tricas e PREVIS√ïES\"\"\"\n",
    "        if self.df is None or self.df.empty:\n",
    "            self.logger.logger.error(\"Dados n√£o carregados. Abortando an√°lise.\")\n",
    "            return\n",
    "\n",
    "        self.logger.logger.info(\"=== INICIANDO AN√ÅLISE SCCS ===\")\n",
    "\n",
    "        # Calcular SCCS vetorizado (tentar paralelo se dispon√≠vel)\n",
    "        self.df['SCCS'] = self.calculator.calculate_sccs_parallel(self.df)\n",
    "\n",
    "        # Calcular din√¢mica de ciclo\n",
    "        self.calculate_cycle_dynamics()\n",
    "\n",
    "        # NOVA FUNCIONALIDADE: Inicializar sistema de previs√£o\n",
    "        self.predictor = CyclePrediction(self.config, self.logger)\n",
    "\n",
    "        # Construir modelo de transi√ß√£o\n",
    "        self.predictor.build_transition_matrix(self.results_df)\n",
    "\n",
    "        # Fazer previs√µes para cada empresa\n",
    "        self.predictions = {}\n",
    "        for empresa in self.results_df['EMPRESA'].unique():\n",
    "            empresa_data = self.results_df[self.results_df['EMPRESA'] == empresa]\n",
    "            if not empresa_data.empty:\n",
    "                latest = empresa_data.iloc[-1]\n",
    "                prediction = self.predictor.predict_next_cycle(empresa, latest)\n",
    "                self.predictions[empresa] = prediction\n",
    "\n",
    "                # Log\n",
    "                self.logger.log_calculation(\n",
    "                    empresa,\n",
    "                    'CYCLE_PREDICTION',\n",
    "                    prediction['confidence'],\n",
    "                    prediction\n",
    "                )\n",
    "\n",
    "        # Detectar regimes de mercado\n",
    "        if len(self.results_df) > 0:\n",
    "            self.results_df['REGIME'] = self.metrics.regime_detection(self.results_df)\n",
    "\n",
    "        # Calcular m√©tricas de risco\n",
    "        if 'SCCS' in self.results_df.columns:\n",
    "            # Calcular retornos do SCCS\n",
    "            sccs_returns = self.results_df.groupby('EMPRESA')['SCCS'].pct_change()\n",
    "            sccs_returns = sccs_returns.dropna()\n",
    "\n",
    "            if len(sccs_returns) > 20:  # Precisa de dados suficientes\n",
    "                self.risk_metrics = self.metrics.calculate_risk_metrics(sccs_returns)\n",
    "                self.logger.logger.info(f\"M√©tricas de risco calculadas: {self.risk_metrics}\")\n",
    "\n",
    "        # Limpar mem√≥ria ap√≥s processamento pesado\n",
    "        self.cleanup_memory()\n",
    "\n",
    "        self.logger.logger.info(\"‚úÖ An√°lise SCCS, m√©tricas avan√ßadas e previs√µes conclu√≠das\")\n",
    "\n",
    "    def calculate_cycle_dynamics(self):\n",
    "        \"\"\"Calcula din√¢mica de ciclo com indicadores avan√ßados\"\"\"\n",
    "        # Preparar dados para an√°lise de ciclo\n",
    "        sccs_data = []\n",
    "\n",
    "        for empresa in self.df['EMPRESA'].unique():\n",
    "            empresa_df = self.df[self.df['EMPRESA'] == empresa].copy()\n",
    "\n",
    "            for idx, row in empresa_df.iterrows():\n",
    "                sccs_data.append({\n",
    "                    'EMPRESA': empresa,\n",
    "                    'DATA': row['DATA'],\n",
    "                    'SCCS': row.get('SCCS', 0),\n",
    "                    'SALES_GROWTH': row.get('SALES_GROWTH', np.nan),\n",
    "                    'RETURN_COM_EQY': row.get('RETURN_COM_EQY', np.nan),\n",
    "                    'OPER_MARGIN': row.get('OPER_MARGIN', np.nan),\n",
    "                    'OPERATING_ROIC': row.get('OPERATING_ROIC', np.nan),\n",
    "                    'CF_FREE_CASH_FLOW': row.get('CF_FREE_CASH_FLOW', np.nan),\n",
    "                    'NET_DEBT_EQTY': row.get('NET_DEBT_EQTY', np.nan)\n",
    "                })\n",
    "\n",
    "        df = pd.DataFrame(sccs_data)\n",
    "\n",
    "        # Calcular dire√ß√£o (m√©dia m√≥vel)\n",
    "        df['DIRECAO'] = df.groupby('EMPRESA')['SCCS'].transform(\n",
    "            lambda x: x.rolling(2, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "        # Calcular acelera√ß√£o (taxa de mudan√ßa)\n",
    "        df['ACELERACAO'] = df.groupby('EMPRESA')['DIRECAO'].diff().fillna(0)\n",
    "\n",
    "        # Calcular momentum (EMA)\n",
    "        df['MOMENTUM'] = df.groupby('EMPRESA')['SCCS'].transform(\n",
    "            lambda x: x.ewm(span=3, adjust=False).mean()\n",
    "        )\n",
    "\n",
    "        # Determinar ciclo\n",
    "        conditions = [\n",
    "            (df['DIRECAO'] >= 0) & (df['ACELERACAO'] >= 0),\n",
    "            (df['DIRECAO'] >= 0) & (df['ACELERACAO'] < 0),\n",
    "            (df['DIRECAO'] < 0) & (df['ACELERACAO'] < 0),\n",
    "            (df['DIRECAO'] < 0) & (df['ACELERACAO'] >= 0)\n",
    "        ]\n",
    "        choices = ['Expans√£o', 'Retra√ß√£o', 'Desacelera√ß√£o', 'Recupera√ß√£o']\n",
    "\n",
    "        df['CICLO'] = np.select(conditions, choices, default='Indefinido')\n",
    "\n",
    "        # Adicionar indicadores de trimestre\n",
    "        df['TRIMESTRE'] = df['DATA'].dt.quarter\n",
    "        df['ANO'] = df['DATA'].dt.year\n",
    "        df['TRIMESTRE_STR'] = df['TRIMESTRE'].astype(str) + 'T' + df['ANO'].astype(str)\n",
    "\n",
    "        # Calcular for√ßa do ciclo\n",
    "        df['CICLO_FORCA'] = np.sqrt(df['DIRECAO']**2 + df['ACELERACAO']**2)\n",
    "\n",
    "        self.results_df = df\n",
    "\n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"Gera relat√≥rio profissional completo COM PREVIS√ïES\"\"\"\n",
    "        if self.results_df.empty:\n",
    "            self.logger.logger.error(\"Nenhum resultado para gerar relat√≥rio\")\n",
    "            return\n",
    "\n",
    "        self.logger.logger.info(\"=== GERANDO RELAT√ìRIO PROFISSIONAL ===\")\n",
    "\n",
    "        # 1. Resumo executivo\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\" RELAT√ìRIO EXECUTIVO - AN√ÅLISE SCCS PROFESSIONAL v4.0\")\n",
    "        print(\"=\"*100)\n",
    "\n",
    "        # 2. M√©tricas gerais\n",
    "        print(\"\\nüìä M√âTRICAS GERAIS\")\n",
    "        print(\"-\"*50)\n",
    "        print(f\"‚Ä¢ Total de empresas analisadas: {self.results_df['EMPRESA'].nunique()}\")\n",
    "        print(f\"‚Ä¢ Per√≠odo de an√°lise: {self.results_df['DATA'].min():%Y-%m-%d} a {self.results_df['DATA'].max():%Y-%m-%d}\")\n",
    "        print(f\"‚Ä¢ Total de observa√ß√µes: {len(self.results_df)}\")\n",
    "\n",
    "        # 3. Top performers\n",
    "        last_results = self.results_df.loc[\n",
    "            self.results_df.groupby('EMPRESA')['DATA'].idxmax()\n",
    "        ].set_index('EMPRESA')\n",
    "\n",
    "        top_performers = last_results.nlargest(10, 'SCCS')[['SCCS', 'CICLO', 'DIRECAO', 'ACELERACAO', 'MOMENTUM']]\n",
    "\n",
    "        print(\"\\nüèÜ TOP 10 EMPRESAS POR SCCS\")\n",
    "        print(\"-\"*50)\n",
    "        print(top_performers.round(2))\n",
    "\n",
    "        # 4. Distribui√ß√£o de ciclos\n",
    "        print(\"\\nüìà DISTRIBUI√á√ÉO DE CICLOS ATUAL\")\n",
    "        print(\"-\"*50)\n",
    "        cycle_dist = last_results['CICLO'].value_counts()\n",
    "        for ciclo, count in cycle_dist.items():\n",
    "            pct = (count / len(last_results)) * 100\n",
    "            print(f\"‚Ä¢ {ciclo}: {count} empresas ({pct:.1f}%)\")\n",
    "\n",
    "        # 5. M√©tricas de risco\n",
    "        if self.risk_metrics:\n",
    "            print(\"\\n‚ö†Ô∏è M√âTRICAS DE RISCO DO PORTFOLIO\")\n",
    "            print(\"-\"*50)\n",
    "            for metric, value in self.risk_metrics.items():\n",
    "                if isinstance(value, float):\n",
    "                    print(f\"‚Ä¢ {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "        # 6. Alertas e anomalias\n",
    "        print(\"\\nüö® ALERTAS E ANOMALIAS\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        # Empresas com mudan√ßas bruscas\n",
    "        sccs_changes = self.results_df.groupby('EMPRESA')['SCCS'].diff()\n",
    "        extreme_changes = sccs_changes[abs(sccs_changes) > sccs_changes.std() * 2]\n",
    "\n",
    "        if not extreme_changes.empty:\n",
    "            print(f\"‚Ä¢ {len(extreme_changes)} mudan√ßas extremas detectadas em SCCS\")\n",
    "            affected_companies = self.results_df.loc[extreme_changes.index, 'EMPRESA'].unique()\n",
    "            print(f\"  Empresas afetadas: {', '.join(affected_companies[:5])}\")\n",
    "\n",
    "        # 7. Hist√≥rico de ciclos\n",
    "        ciclo_pivot = self.results_df.pivot_table(\n",
    "            index='EMPRESA',\n",
    "            columns='TRIMESTRE_STR',\n",
    "            values='CICLO',\n",
    "            aggfunc='first'\n",
    "        )\n",
    "\n",
    "        if not ciclo_pivot.empty:\n",
    "            ciclo_pivot = ciclo_pivot.iloc[:, -8:]  # √öltimos 8 trimestres\n",
    "\n",
    "            print(\"\\nüìÖ HIST√ìRICO DE CICLOS (√öltimos 8 Trimestres)\")\n",
    "            print(\"-\"*50)\n",
    "            print(ciclo_pivot.head(10))\n",
    "\n",
    "        # 8. NOVA SE√á√ÉO: PREVIS√ïES DE PR√ìXIMO CICLO\n",
    "        if hasattr(self, 'predictions') and self.predictions:\n",
    "            print(\"\\nüîÆ PREVIS√ïES DE PR√ìXIMO CICLO\")\n",
    "            print(\"-\"*50)\n",
    "\n",
    "            # Top 10 empresas com maior confian√ßa\n",
    "            sorted_preds = sorted(\n",
    "                self.predictions.items(),\n",
    "                key=lambda x: x[1]['confidence'],\n",
    "                reverse=True\n",
    "            )[:10]\n",
    "\n",
    "            for empresa, pred in sorted_preds:\n",
    "                print(f\"\\n{empresa}:\")\n",
    "                print(f\"  Ciclo Atual: {pred['current_cycle']}\")\n",
    "                print(f\"  ‚Üí Pr√≥ximo Ciclo: {pred['predicted_cycle']}\")\n",
    "                print(f\"  Confian√ßa: {pred['confidence']*100:.1f}%\")\n",
    "                print(f\"  SCCS: {pred['indicators']['SCCS']:.2f}\")\n",
    "\n",
    "            # Resumo das previs√µes\n",
    "            print(\"\\nüìä RESUMO DAS PREVIS√ïES\")\n",
    "            print(\"-\"*30)\n",
    "            pred_counts = {}\n",
    "            for _, pred in self.predictions.items():\n",
    "                cycle = pred['predicted_cycle']\n",
    "                pred_counts[cycle] = pred_counts.get(cycle, 0) + 1\n",
    "\n",
    "            for cycle, count in sorted(pred_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                pct = (count / len(self.predictions)) * 100\n",
    "                print(f\"  {cycle}: {count} empresas ({pct:.1f}%)\")\n",
    "\n",
    "        # 9. Exportar arquivos\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "        # Resumo executivo\n",
    "        summary_filename = f'sccs_executive_summary_{timestamp}.csv'\n",
    "        top_performers.to_csv(summary_filename)\n",
    "\n",
    "        # Adicionar previs√µes ao DataFrame de resumo se existirem\n",
    "        if self.predictions:\n",
    "            predictions_df = pd.DataFrame.from_dict(self.predictions, orient='index')\n",
    "            predictions_filename = f'sccs_predictions_{timestamp}.csv'\n",
    "            predictions_df.to_csv(predictions_filename)\n",
    "            print(f\"   ‚Ä¢ Previs√µes: {predictions_filename}\")\n",
    "\n",
    "        # Dados completos\n",
    "        if self.config.OUTPUT_FORMAT == 'excel':\n",
    "            with pd.ExcelWriter(f'sccs_analysis_{timestamp}.xlsx', engine='openpyxl') as writer:\n",
    "                self.results_df.to_excel(writer, sheet_name='An√°lise Completa', index=False)\n",
    "                top_performers.to_excel(writer, sheet_name='Top Performers')\n",
    "                ciclo_pivot.to_excel(writer, sheet_name='Hist√≥rico Ciclos')\n",
    "\n",
    "                # Adicionar previs√µes\n",
    "                if self.predictions:\n",
    "                    predictions_df.to_excel(writer, sheet_name='Previs√µes')\n",
    "\n",
    "                # Adicionar m√©tricas de risco\n",
    "                if self.risk_metrics:\n",
    "                    pd.DataFrame([self.risk_metrics]).to_excel(\n",
    "                        writer, sheet_name='M√©tricas de Risco', index=False\n",
    "                    )\n",
    "        elif self.config.OUTPUT_FORMAT == 'parquet':\n",
    "            self.results_df.to_parquet(f'sccs_analysis_{timestamp}.parquet')\n",
    "        else:  # CSV padr√£o\n",
    "            self.results_df.to_csv(f'sccs_analysis_{timestamp}.csv', index=False)\n",
    "\n",
    "        # Exportar log de auditoria\n",
    "        self.logger.export_audit_trail(f'audit_trail_{timestamp}.json')\n",
    "\n",
    "        print(f\"\\n‚úÖ Relat√≥rios exportados com sucesso!\")\n",
    "        print(f\"   ‚Ä¢ Resumo: {summary_filename}\")\n",
    "        print(f\"   ‚Ä¢ An√°lise completa: sccs_analysis_{timestamp}.{self.config.OUTPUT_FORMAT}\")\n",
    "        print(f\"   ‚Ä¢ Auditoria: audit_trail_{timestamp}.json\")\n",
    "\n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"Executa pipeline completo com todas as melhorias\"\"\"\n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*100)\n",
    "            print(\" SCCS ANALYSIS PIPELINE v4.0 - PROFESSIONAL EDITION\")\n",
    "            print(\"=\"*100)\n",
    "\n",
    "            # 1. Instalar pacotes\n",
    "            print(\"\\nüì¶ Etapa 1: Verificando depend√™ncias...\")\n",
    "            PackageManager.install_packages(['core', 'financial'])\n",
    "\n",
    "            # 2. Carregar dados\n",
    "            print(\"\\nüìä Etapa 2: Carregando e validando dados...\")\n",
    "            if not self.load_and_prepare_data():\n",
    "                raise Exception(\"Falha ao carregar dados\")\n",
    "\n",
    "            # 3. Executar an√°lise\n",
    "            print(\"\\nüî¨ Etapa 3: Executando an√°lise SCCS, m√©tricas e previs√µes...\")\n",
    "            self.run_analysis()\n",
    "\n",
    "            # 4. Gerar visualiza√ß√µes\n",
    "            print(\"\\nüìà Etapa 4: Gerando visualiza√ß√µes...\")\n",
    "\n",
    "            # Dashboard interativo\n",
    "            self.dashboard = self.visualizer.create_interactive_dashboard(\n",
    "                self.results_df,\n",
    "                self.risk_metrics\n",
    "            )\n",
    "\n",
    "            if self.dashboard:\n",
    "                # Salvar dashboard\n",
    "                try:\n",
    "                    import plotly.io as pio\n",
    "                    pio.write_html(\n",
    "                        self.dashboard,\n",
    "                        f'sccs_dashboard_{datetime.now():%Y%m%d_%H%M%S}.html'\n",
    "                    )\n",
    "                    print(\"   ‚úÖ Dashboard interativo salvo\")\n",
    "                except:\n",
    "                    print(\"   ‚ö†Ô∏è N√£o foi poss√≠vel salvar o dashboard\")\n",
    "\n",
    "            # Gr√°ficos de ciclo para top empresas\n",
    "            top_empresas = self.results_df.groupby('EMPRESA')['SCCS'].mean().nlargest(5).index\n",
    "\n",
    "            for empresa in top_empresas:\n",
    "                print(f\"\\n   Gerando gr√°fico de ciclo para {empresa}...\")\n",
    "                self.visualizer.create_cycle_plot_enhanced(empresa, self.results_df)\n",
    "\n",
    "            # 5. Gerar relat√≥rio\n",
    "            print(\"\\nüìù Etapa 5: Gerando relat√≥rio profissional...\")\n",
    "            self.generate_comprehensive_report()\n",
    "\n",
    "            print(\"\\n\" + \"=\"*100)\n",
    "            print(\" ‚úÖ AN√ÅLISE CONCLU√çDA COM SUCESSO!\")\n",
    "            print(\" üîÆ SISTEMA DE PREVIS√ÉO DE CICLOS ATIVADO\")\n",
    "            print(\"=\"*100)\n",
    "\n",
    "            return self.results_df, self.risk_metrics, self.dashboard, self.predictions\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error('run_complete_pipeline', e)\n",
    "            print(f\"\\n‚ùå ERRO CR√çTICO: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# =============================================================================\n",
    "# FUN√á√ÉO PRINCIPAL\n",
    "# =============================================================================\n",
    "def main():\n",
    "    \"\"\"Fun√ß√£o principal para execu√ß√£o do pipeline\"\"\"\n",
    "\n",
    "    # Configurar warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # URL da planilha\n",
    "    sheet_url = \"https://docs.google.com/spreadsheets/d/1SWNjeO7DcYnpYIEbT32Jh2R5un_5b5zP3uoXEH2PvbU/edit?usp=sharing\"\n",
    "\n",
    "    # Criar configura√ß√£o customizada (opcional)\n",
    "    config = Config()\n",
    "    config.LOG_LEVEL = 'INFO'\n",
    "    config.OUTPUT_FORMAT = 'csv'  # ou 'excel', 'parquet'\n",
    "\n",
    "    # Criar e executar pipeline\n",
    "    pipeline = SCCSAnalysisPipeline(sheet_url, config)\n",
    "\n",
    "    try:\n",
    "        results_df, risk_metrics, dashboard, predictions = pipeline.run_complete_pipeline()\n",
    "\n",
    "        # Retornar resultados para uso posterior\n",
    "        return {\n",
    "            'results': results_df,\n",
    "            'risk_metrics': risk_metrics,\n",
    "            'dashboard': dashboard,\n",
    "            'predictions': predictions,\n",
    "            'pipeline': pipeline\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erro na execu√ß√£o: {e}\")\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# EXECU√á√ÉO\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()\n",
    "\n",
    "    if results:\n",
    "        print(\"\\nüìä Pipeline dispon√≠vel em 'results' para an√°lises adicionais\")\n",
    "        print(\"   ‚Ä¢ results['results']: DataFrame com an√°lise completa\")\n",
    "        print(\"   ‚Ä¢ results['risk_metrics']: M√©tricas de risco calculadas\")\n",
    "        print(\"   ‚Ä¢ results['dashboard']: Dashboard interativo (se dispon√≠vel)\")\n",
    "        print(\"   ‚Ä¢ results['predictions']: Previs√µes de pr√≥ximo ciclo para cada empresa\")\n",
    "        print(\"   ‚Ä¢ results['pipeline']: Objeto pipeline para an√°lises customizadas\")\n",
    "\n"
   ]
  }
 ]
}